---
title: "Appendix 2"
date: "Last updated: `r Sys.Date()`"
toc: true
header-includes:
  - \usepackage{float}
output: 
  rmarkdown::pdf_document:
    fig_caption: yes
    includes:
      in_header: preamble.tex
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, messages = FALSE, warning = FALSE, cache = TRUE)
```

<!-- ## --------------------------- -->
<!-- ## Objective: -->
<!-- ##    - Imports species and covariate data -->
<!-- ##    - Scales covariates -->
<!-- ##    - Sets up data for NIMBLE -->
<!-- ## -->
<!-- ## Input: -->
<!-- ##    - functions/FXN-MVPv1.1.R -->
<!-- ##    - code/03-species-models/MVPv1.csv -->
<!-- ## -->
<!-- ## Output: -->
<!-- ##    - setup_BLOCK.rdata (saves environment for import to fit model) -->
<!-- ## -->
<!-- ## --------------------------- -->

\newpage
So you wanna fit an integrated species distribution model? You've come to the right place!

# NEED SOME SORT OF INTRO HERE.

Our workflow is separated into four parts.

- First, the data needs to be in the correct format. We do not provide a script for this, as each dataset begins in a unique format. Instead, we provide a description of the format that the analysis requires.
- The first script sets up the range, imports species and covariate data, scales covariates, and sets everything up for NIMBLE.
- The second script fits the NIMBLE model.
- The third script summarizes the NIMBLE output and creates output figures.


Let's first load the libraries we'll need to set up, visualize, fit, and summarize our data.

```{r libraries, results = 'hide', message = F}
# Load libraries ----
library(tidyverse)
library(sf)
library(nimble)

## To install SpFut.flexiSDM or check for updates, use the commented code below:
# remotes::install_github("rileymummah/SpFut.flexiSDM", build_vignettes = T)
library(SpFut.flexiSDM)
library(SpFut.covariates)

## There are two custom functions that are needed to fit the data in NIMBLE
source("code/FXN-nimbleParallel.R")
```

# Part 0: Data formatting

Data must be in the following structure to enter models. This structure is suitable for Presence-only (PO), Detection/non-detection (DND), and count (Count) data. Other data types can usually be reduced into one of these types (e.g., capture-mark-recapture can be reduced to Count or DND). In anticipation of adding a specific observation model for CMR and time-to-detection data in the future, we include `individual.id` and `time.to.detection` as required columns in the data format.

We use a long format, where there is a unique row for each age (if recorded) of each species in each pass of each sampling event (defined as a visit to a site). There are 13 required columns. Any other information recorded in the field that relates to detection probability (e.g., duration of survey, area surveyed) should be included as additional columns. There should be no empty cells. Use NA (not NULL, -999, etc.) to indicate missing data.


| Column Name   | Format     | Appropriate values   | Notes            |
|---------------|------------|----------------------|------------------|
| site.id       | character  | alphanuermic string  | Each site.id is associated with exactly one set of coorinates|
| lat           | numeric    | WGS84                | Each set of coordinates is associated with exactly one site.id|
| lon           | numeric    | WGS84                | Each set of coordinates is associated with exactly one site.id|
| day           | numeric    | 1-31                 | If day was not recorded, use NA |
| month         | numeric    | 1-12                 | If month was not recorded, use NA |
| year          | numeric    | 4 digit year         | If year was not recorded, use NA |
| survey.conducted | numeric | 0, 1                 | Indicates whether or not a survey was conducted (e.g., if there was no water at an intended survey location, survey.conducted = 0). For all PO records, survey.conducted = 1 |
|survey.id      | numeric    | any number           | All observations from a single visit to a single site have the same survey.id  |
| data.type     | character  | `PO`, `DND`, `count`       | data.type may vary across species or age class (e.g. adults are count but larvae are DND) |
| pass.id       | numeric    | any number           | Within a survey.id (i.e., a visit to a site), each pass should have a unique pass.id. All observations within a pass should have the same pass.id |
| species       | character  | species identifier   | We use 4- or 6-digit codes. Any value can be used, as long as it is consistent across all datasets |
| age           | character  | `egg`, `egg mass`, `larva`, `juvenile`, `metamorph`, `adult`, `unknown`, `NR` | Use NR if age classes were not recorded. Do not use NA |
| individual.id | character  | alphanumeric string  | If no individuals were seen (count = 0), then individual.id = NA. If individuals were seen (count = 1) but not marked (too small, escaped, etc.), then individual.id = 0. | Applies only to CMR, otherwise use NA. |
| time.to.detect | numeric   | any number           | Use NA if time to detection was not recorded |
| count          | numeric   | If count: 0-Inf; If DND or CMR: 0/1/2; If PO: 1 | Must be a specific value, not range of numbers (e.g., 10-50); for DND and CMR, 0 = not detected, 1 = detected, 2 = maybe detected |

*A Note on survey.id and pass.id:* To make sure all data enter the model correctly, we need to know which records were part of the same sampling event. A sampling event is a specific visit to a specific site. Sampling events are identified with `survey.id`. During a sampling event, multiple observations (“passes”) may be made, e.g., multiple observers, multiple passes by a single observer, multiple audio recordings, multiple water samples for eDNA. Generally, site.id, day, month, and year define a unique sampling event. The exception is if passes occur overnight: passes that occur after midnight should be given the same survey.id as passes that occur before midnight.

All passes within a sampling event get the same survey.id. All passes within a sampling event get a unique pass.id.

Example: a double observer visual encounter survey. Both observations at Site A on Day 1 get survey.id = 1, and the observations by each observer get a pass.id that is unique within the survey.id (1 and 2). All observations at site B on Day 1 get survey.id = 2, and each observation gets a unique pass.id (1 and 2). Site A is revisited on Day 2, and all observations get survey.id = 3, and each observation gets a unique pass.id (1 and 2).

Example: five audio recordings are taken at each visit to a site. All five recordings at Site A on Day 1 get survey.id = 1, and each recording gets a pass.id from 1 to 5. All recordings at Site B on Day 1 get survey.id = 2, and each recording gets a pass.id from 1 to 5. Site A is revisited on Day 2, and all recordings get survey.id = 3, and each recording gets a pass.id from 1 to 5.

![Distribution of species data across the subrange](~/GitHub/iSDM-framework/appendix2/annotateddata.png)


# Part 1: 01-flexiSDM.R

## Load and define model specifications

Each model requires a variety of specifications. Rather than enter them manually in the code, we find it easier to manage multiple models by storing all specifications in a .csv file that is read in to provide input values. We call our csv file `model-specs.csv`. Each row represents a model, so you can easily see and edit the parameters used for each model. Let's load the .csv and take a look at how we structured it.

```{r model-specs}
mods <- read.csv("code/model-specs.csv")

head(mods)
```

The following table describes the column names, data type, and description found in `model-specs.csv`. Not all columns are required depending how you intend to fit the model (i.e., local computer vs. high performance computing cluster).

| Column Name     | Type      | Recommended value  | Description                              |
|-----------------|-----------|--------------------|------------------------------------------|
| number          | numeric   | 1                  | Model number                             |
| sp.code         | character | species identifier | Species code, must match the species code used in the species data |
| model           | character | short descriptor   | Model name                               |
| region.sub      | logical   | F, unless troubleshooting | Only use the centroid of the region with a radius equal to the buffer size? |
| buffer          | numeric   | 50000              | Size of buffer around range edge or centroid (in m)  |
| cont.grid       | logical   | T                  | Should the grid be restricted to continuous cells? (This will remove any groups of non-contiguous cells) |
| coarse.grid     | logical   | T, unless small region | Should a coarse grid be used for the spatial model? |
| lon.lo          | numeric   | empty, unless troubleshooting | Low longitude value to restrict the range |
| lon.hi          | numeric   | empty, unless troubleshooting |  High longitude value to restrict the range |
| lat.lo          | numeric   | empty, unless troubleshooting |  Low latitude value to restrict the range |
| lat.hi          | numeric   | empty, unless troubleshooting |  High latitude value to restrict the range |
| year.start      | numeric   | 1994               | Starting year of data to include         |
| year.end        | numeric   | current year       | Ending year of data to include           |
| filter.region   | logical   | F to see if species is recorded outside of range | Should data outside of the region be removed? Note that this is only for mapping purposes; data that are outside of the region are never used to fit the model          |
| spat.thin        | logical   | T                  |Should the PO data be spatially thinned?         |
| coordunc        | numeric   | 1000            | The level of acceptable coordinate uncertainty (meters, for PO datasets that record coordinate uncertainty)    |
| coordunc_na.rm  | logical   | T               | Should PO data with coordinate uncertainty = NA be removed? (for PO datasets that record coordinate uncertainty)|
| covs.PO         | character | human density   | A comma-separated list of covariates for modeling non-iNaturalist PO sampling effort. These must match the covariate names in the data. |
| covs.inat       | character | human density   | A comma-separated list of covariates for modeling iNaturalist sampling effort. These must match the covariate names in the data. |
| covs.lin        | character | based on species ecology | A comma-separated list of linear covariates for modeling species relative abundance. These must match the covariate names in the data. |
| covs.quad       | character | based on species ecology | A comma-separated list of quadratic covariates for modeling species relative abundance. These must match the covariate names in the data. |
| check.covs      | logical   | T               | Should covariate selection remove multicollinearity? |
| Bprior          | numeric or character | dnorm(0, 1) | Assign a prior distribution in BUGS language for the $\beta$ coefficients in the distribution model (e.g., dnorm(1,0))
| sp.auto         | logical   | T               | Should a spatial model be included?      |
| zero_mean       | logical   | T               | Should a zero mean assumption be included in the spatial model? |
| tau             | numeric or character | 1    | Assign a fixed value or a prior distribution in BUGS language for precision ($\tau$; e.g., dgamma(5,5)) |
| iter            | numeric   | depends on computational power | The number of iterations to run an MCMC chain (the first 75% will be discarded as a burn-in)    |
| thin            | numeric   | 5               | Thin the MCMC chains by this parameter            |
| project         | numeric   | 0               | The number of future projections (otherwise, NA)  |
| block.rows      | numeric   | 5               | The number of rows for cross-validation blocks    |
| block.cols      | numeric   | 5               | The number of columns for cross-validation blocks |
| block.folds     | numeric   | 3               | The number of folds to group cross-validation blocks into       |
| mem.nim         | numeric   | 30000           | Memory allocation (in MB) for NIMBLE. Can be omitted if not using an HPC. |
| mem.sum         | numeric   | 40000           | Memory allocation (in MB) for parallelized summarization. Can be omitted if not using an HPC. |

We have set up the scripts in the flexiSDM workflow so that a minimum number of parameters needs to be changed among them. To run this script in full (using our file arrangement), you only need to edit the following parameters:

```{r param.edit}
nums.do <- 4 # model number to run
fold <- 'none' # CV fold to exclude ('none', 1, 2, or 3)
local <- 1 # are you running the code locally (1) or on an HPC (0)?
```


For the purposes of this demonstration, we're going to fit model 3, which uses the centroid of the Cascades frog (RACA) range with a 50km buffer. This will allow us to easily visualize the datasets and spatial grid, while maintaining local computational capabilities. You can see all of the specifications for this model in the third row of `mods`.



Now that we have loaded our model specifications, we can use them to define a series of inputs for the setup script.

```{r mods}
## Set up model variables
mods <- filter(mods, number %in% nums.do)

## Get variables for model from model-specs.csv
number <- mods$number[1] # model number
sp.code <- mods$sp.code[1] # species code
model <- mods$model[1] # model name
sp.auto <- mods$sp.auto[1] # include spatial autocorrelation?
coarse.grid <- mods$coarse.grid[1] # use a coarse grid for the spatial effect?
cont.grid <- mods$cont.grid[1] # restrict to only a continuous grid?
year.start <- mods$year.start[1] # start year for data
year.end <- mods$year.end[1] # end year for data
buffer <- mods$buffer[1] # buffer size (m)
filter.region <- mods$filter.region[1] # filter data to the region?
spat.thin <- mods$spat.thin[1] # include spatial thinning for PO data?
coordunc <- mods$coordunc[1] # level of coordinate uncertainty to include (m)
coordunc_na.rm <- mods$coordunc_na.rm[1] # include coordinate uncertainty = NA?
block.folds <- mods$block.folds[1] # how many groups of blocks?
block.rows <- mods$block.rows[1] # how many rows of blocks?
block.cols <- mods$block.cols[1] # how many columns of blocks?
fold.out <- fold # define the fold to setup

if (fold.out == "none") {
  foldname <- "full" # rename the fold to 'full' if not doing cross-validation
} else {foldname <- fold.out} # otherwise, keep the fold number

## ICAR parameters
zero_mean <- mods$zero_mean[1] # zero mean assumption?
tau <- mods$tau[1] # precision (tau) can be a fixed value or a prior

## MCMC parameters
iter <- mods$iter[1] # number of MCMC iterations
thin <- mods$thin[1] # number to thin by
burnin <- floor(iter*0.75) # burnin to discard

## Change of region
region.sub <- mods$region.sub[1] # only use the centroid + buffer?
lat.hi <- mods$lat.hi[1] # high value of latitude range
lat.lo <- mods$lat.lo[1] # low value of latitude range
lon.hi <- mods$lon.hi[1] # high value of longitude range
lon.lo <- mods$lon.lo[1] # low value of longitude range

## Future projections
project <- mods$project[1] # how many projections?
if (fold.out != "none") {
  project <- 0 # no projections for cross-validation models
}

## Covariates
# non-iNaturalist PO covariates
covs.PO <- unlist(str_split(mods$covs.PO[1], pattern = ", "))
# iNaturalist covariates
covs.inat <- unlist(str_split(mods$covs.inat[1], pattern = ", "))
# linear covariates for distribution model
covs.lin <- unlist(str_split(mods$covs.lin[1], pattern = ", "))
# quadratic covariates for distribution model
covs.quad <- unlist(str_split(mods$covs.quad[1], pattern = ", "))
check.covs <- mods$check.covs[1] # covariate selection to remove multicollinearity? (T/F)

## Define the prior for the distribution model coefficients
Bprior <- mods$Bprior[1]

## Combine linear and quadratic distribution covariates into covs.z
covs.z <- c(covs.lin, covs.quad)
if ("" %in% covs.z) {
  covs.z <- covs.z[-which(covs.z == "")]  
}
if (NA %in% covs.z) {
  covs.z <- covs.z[-which(is.na(covs.z))]
}
```


We're almost ready to set up the region and load data. Let's first ensure that we have output and data folders specific to the model we are fitting to store some of these objects.

```{r output-folder}
## Make output folder
out.dir <- paste0("outputs/", number, "_", sp.code, "_", model, "/")
if (dir.exists(out.dir) == F) {
  dir.create(out.dir)
}

## Make data folder
data.dir <- paste0("data/", number, "_", sp.code, "_", model, "/")
if (dir.exists(data.dir) == F) {
  dir.create(data.dir)
}
```




## Step 1: Define the region

We define the region of inference using the GAP and IUCN ranges. Other boundaries could be used to create the limits of the range. We saved the IUCN and GAP range boundaries in folders in `data/sp.code`, so that they can be called by the `get_range()` function. We have also downloaded the US Census state boundary shapefile (`cb_2018_us_state_500k.shp`) to dictate the national and state borders. We have already overlaid a continental US hexbin grid (`conus.grid`). Unfortunately, this file is too large to upload to GitHub, so we demonstrate how we created the region and provide the file in `data.dir/region.rds` to bypass this step.

```{r ranges, results = 'hide'}
## Generating the region requires files that are too big for GitHub just read it in
if (file.exists(paste0(data.dir, "region.rds"))) {
  region <- read_rds(file = paste0(data.dir, "region.rds"))
} else {
  ## Load ranges
  range.path <- c(paste0("data/", sp.code, "/GAP/"),  
                  paste0("data/", sp.code, "/IUCN/"))
  range.name <- c("GAP", "IUCN")
  rangelist <- get_range(range.path,
                         range.name,
                         crs = 4326)
  # We use GAP and IUCN ranges, but you can use any polygons. Just enter the path to where you store the shape files
  
  ## USA boundary
  exclude <- c("Alaska", "Hawaii", "Commonwealth of the Northern Mariana Islands",
               "American Samoa", "United States Virgin Islands", "Guam", "Puerto Rico")
  usa <- st_read("../species-futures/data/USA/maps/cb_2018_us_state_500k/cb_2018_us_state_500k.shp") %>%
    filter((NAME %in% exclude) == F) %>%
    st_union()
  ## CONUS grid
  load("../species-futures/data/USA/grid-and-huc.rdata")
  
  ## Region
  region <- make_region(rangelist,
                        buffer = buffer,
                        sub = region.sub,
                        boundary = usa, # crop study region to this boundary
                        grid = conus.grid, # You can use any grid to delineate spatial units
                        rm.clumps = T, # remove small clumps of cells?
                        clump.size = 50, # size of clumps to remove, if rm.clumps = T
                        continuous = cont.grid)
  
  write_rds(region, file = paste0(data.dir, "region.rds"))
}
```

There is an additional step here if you intend to use a coarse spatial grid for the spatial model. Using a coarse grid greatly reduces computation time, which may be of concern for large-range species. We must redefine the spatial grid by grouping neighboring hexbins together in sets of seven via the `make_spatkey()` function. Otherwise, `spatRegion` is set to NULL.

```{r spatRegion}
## If using the coarse spatial grid, redefine the grid and plot the resulting grid.
if (coarse.grid == T) {
  spatRegion <- suppressWarnings(make_spatkey(region$sp.grid))
  
  # Print spatial grid
  if (fold.out == 'none') {
    pl <- ggplot(spatRegion$spat.grid) + geom_sf() + theme_bw()
    ggsave(pl, file = paste0(out.dir, "2_inputmap-e_spatGrid.jpg"), height = 8, width = 10)
  }
} else {
  spatRegion <- NULL
}
```


Finally, identify which cells in the grid fall into which states. Add a column to `region$sp.grid` indicating whether each cell falls into one or more states. These will be used later.
```{r stategrid}
statemap <- ne_states(country = c("Canada", "Mexico", "United States of America"),
                      returnclass = "sf")
stategrid <- get_state_grid(region, statemap)

xstate <- stategrid %>% group_by(conus.grid.id) %>% summarize(nstate = n()) %>% filter(nstate > 1) %>% pull(conus.grid.id)
region$sp.grid <- region$sp.grid %>%
  mutate(nstate = case_when(conus.grid.id %in% xstate ~ "multi",
                            T ~ "single"))
```



## Step 2: Designate training and testing data

We have set up the code so that this section needs to be run even if you do not intend to exclude data for cross-validation. If you do are not excluding any blocks, all of the data are considered "train" data. Otherwise, the data are split into "train" and "test" data depending on the cross-validation fold (1, 2, or 3) that is excluded.

```{r cv-blocks}
spatblocks <- make_CV_blocks(region, rows = block.rows, cols = block.cols, 
                             k = block.folds)

if (fold.out == "none") { 
  # If fitting the full model, then there are no test data
  test.i <- c()            
  train.i <- region$sp.grid$conus.grid.id
  
} else {
  
  ## Set up cross validation blocks
  block1 <- spatblocks %>% filter(folds == fold.out)
  
  ## Find grid.ids for test fold, everything else is in the training fold
  test.i <- st_intersection(region$sp.grid, block1) %>%
    pull(conus.grid.id) %>%
    unique()
  
  train.i <- filter(region$sp.grid, conus.grid.id %in% test.i == F) %>%
    pull(conus.grid.id)
}
```


After the cross-validation blocks are assembled, the cells are assigned to "train" and "test" groups in a `gridkey`. This `gridkey` also contains `grid.id`, which allows us to index the data in NIMBLE.


```{r gridkey}
# Make gridkey ----
gridkey <- select(region$sp.grid, conus.grid.id) %>%
  st_drop_geometry() %>%
  mutate(grid.id = 1:nrow(.),
         group = case_when(conus.grid.id %in% train.i ~ "train",
                           conus.grid.id %in% test.i ~ "test"))
```




## Step 3: Load species data

We're now ready to load the species data. We have assembled a full amphibian species list (`model-specieslist.csv`) from which we pull out the common name, any species code that could refer to the species of interest, and the scientific name. We demonstrate how we do this with our list, but these values could be set manually or pulled from a different list. We additionally define the species type (e.g., Frog/Toad or Salamander) based off the genus name. This delineation is used later in the covariate data section.

```{r spp-codes}
## Define species codes
codeKey <- read.csv("data/model-specieslist.csv")

## Common name
common <- codeKey %>%
  filter(DS.code == sp.code) %>%
  pull(SSAR.common)
common

## All species codes used (some species are complexes or have subspecies so multiple codes exist)
sp.code.all <- codeKey %>%
  filter(DS.code == sp.code) %>%
  pull(all.codes)
sp.code.all

## Scientific name
sciname <- codeKey %>%
  filter(DS.code == sp.code) %>%
  pull(SSAR.scientific)
sciname

## Save genus name to assign Frog/Toad or Salamander
spp.type <- codeKey %>%
  filter(DS.code == sp.code) %>%
  pull(spp.type)
spp.type
```

Next we will create a dataframe containing information about each data source. We define ours in a CSV file (`00-data-summary-flexiSDM.csv`) because we have hundreds of datasets to filter through for different species. This process could also be done by manually creating a dataframe with the same column names. The necessary columns are: 

-`file.name` (how the file is named in the source folder)
-`file.label` (the label for the data source)
-`data.type` (Count, DND, or PO)
-`PO.extent` (does the presence-only span the continent or a particular state; only required for PO)
-`covar.mean` (detection covariates that should be averaged across passes) 
-`covar.sum` (detection covariates that should be summed across passes)

For additional clarification, sometimes it is more appropriate to summarize detection covariates by averaging than summing and vice versa. For example, the number of survey minutes should be summed across passes but the temperature during the survey should be averaged. 


```{r species-data}
# get all files that have data for that species
allfiles <- read.csv("data/00-data-summary-flexiSDM.csv") %>%
  filter(Species == sp.code) %>%
  rename(file.name = Data.Swamp.file.name,
         file.label = Name,
         covar.mean = Covar.mean,
         covar.sum = Covar.sum,
         data.type = Type.true) %>%
  select(file.name, file.label, covar.mean, covar.sum, 
         data.type, PO.extent)

head(allfiles)
```

We can now use `allfiles` to load all of the species data sources. We have developed the function `load_species_data()` which takes the species code(s), `allfiles`, the region, and information about filtering, start/end dates, and spatial uncertainty to load and filter the data appropriately. The number of observations removed from each data source and the reasons for removal are provided as console output to the user. 


```{r load-species-data}
species.data <- load_species_data(sp.code,
                                  sp.code.all,
                                  file.info = allfiles,
                                  file.path = "data/data-ready/",
                                  region = region, 
                                  filter.region = filter.region,
                                  year.start = year.start,
                                  year.end = year.end,
                                  coordunc = coordunc,
                                  coordunc_na.rm = coordunc_na.rm,
                                  spat.thin = spat.thin,
                                  keep.conus.grid.id = gridkey$conus.grid.id[which(gridkey$group == "train")])

```




## Step 4: Plot species data

Next let's plot the species data to visualize where the data occur in the subrange.  

```{r plot-species-data-samples}
## Quick title to be used across figures
if (fold == "none") {
  title <- ", full model"
} else {
  title <- paste0(", excluding fold ", fold)
}

## Plot data samples
out <- map_species_data(region = region,
                        species.data = species.data,
                        year.start = year.start,
                        year.end = year.end,
                        plot = "samples",
                        plot.region = T,
                        details = F,
                        title = paste0(common, " (", sp.code, ")", title))

## Save plot
ggsave(out$plot, file = paste0(out.dir, "2_inputmap-a_data-", foldname, ".jpg"),
       height = 8, width = 10)
```


![Distribution of species data across the subrange](~/GitHub/iSDM-framework/outputs/3_RACA_subrange/2_inputmap-a_data-full.jpg)

We can also take a look at the distribution of data across the cross-validation folds.


```{r cv-plots}
## Cross-validation fold-specific plots
if (fold.out != "none") {
  
  out <- map_species_data(region = region,
                          species.data = species.data,
                          year.start = year.start,
                          year.end = year.end,
                          plot = "samples",
                          blocks = spatblocks[which(spatblocks$folds == fold),],
                          plot.region = T,
                          details = F,
                          title = paste0(common, " (", sp.code, ")", title))
  ggsave(out$plot, file = paste0(out.dir, "2_inputmap-c_blocks-", foldname, ".jpg"),
         height = 8, width = 10)
} else {
  # Plot
  out <- map_species_data(region = region,
                          species.data = species.data,
                          year.start = year.start,
                          year.end = year.end,
                          plot = "samples",
                          blocks = spatblocks,
                          plot.region = T,
                          details = F,
                          title = paste0(common, " (", sp.code, ")", title))
  ggsave(out$plot, file = paste0(out.dir, "2_inputmap-c_blocks-", foldname, ".jpg"),
         height = 8, width = 10)
}
```

![Distribution of data with overlaid cross-validation blocks](~/GitHub/iSDM-framework/outputs/3_RACA_subrange/2_inputmap-c_blocks-full.jpg)

## Step 5: Load covariate data

We're now ready to set up the covariate data. The dataframe `covar` needs to contain all covariates needed for the distribution model (`covs.z`), as well as all covariates needed for the PO effort models (`covs.PO` and `covs.inat`). These covariates vary across species. Any method can be used to derive the covariates, as long as it is aggregated to the spatial unit and contains a column for `conus.grid.id`.

We download and process covariates using functions from the `SpFut.covariates` package (ISSUE - cite). These functions consist of wrappers for other data sources (e.g., National Hydrography Database and  the `geodata` R package (ISSUE - cite)) that download and format the data for our purposes. See the documentation for the `SpFut.covariates` package for more information on how to use the functions properly.

Note: Downloading and assembling the covariate data can take significant time depending on the size of the range and your internet connection. We demonstrate how we downloaded the data below but provide the `covariates.rds` file to proceed to the next step.

```{r cov-data}

# ISSUE - re-download covariates with CRAN package
if (sp.code == "RACA") {
  
  if (file.exists(paste0(data.dir, "covariates.rds"))) {
    covar <- read_rds(paste0(data.dir, "covariates.rds"))
  } else {
    
    # Note that elevation data must be downloaded before running get_elevation()
    # We have downloaded the elevation and waterbody data outside of this repo
    # See documentation for details.
    tri <- get_elevation(locs = region$sp.grid, path = "../species-futures/data/USA/",
                         id.label = "conus.grid.id")
    waterbody <- get_waterbodies(locs = region$sp.grid, path = "../species-futures/data/USA/",
                                 id.label = "conus.grid.id")
    
    footprint <- get_footprint(locs = region$sp.grid, id.label = "conus.grid.id")
    climate <- get_climate(locs = region$sp.grid, id.label = "conus.grid.id")
    traveltime <- get_traveltime(locs = region$sp.grid, id.label = "conus.grid.id")
    
    covar <- full_join(tri, footprint, by = "conus.grid.id") %>%
      full_join(climate, by = "conus.grid.id") %>%
      full_join(waterbody, by = "conus.grid.id") %>%
      full_join(traveltime, by = "conus.grid.id") %>%
      mutate(sqrtarea_small = sqrt(area_small),
             sqrtarea_medium = sqrt(area_medium)) %>%
      select(conus.grid.id, sqrtarea_small, sqrtarea_medium, 
             footprint, TRI, tmin, traveltime)
    
    covar <- covar[order(match(covar$conus.grid.id, region$sp.grid$conus.grid.id)),]
    write_rds(covar, file = paste0(data.dir, "covariates.rds"))
    
  }
  
}


if (sp.code == "GPOR") {
  
  if (file.exists(paste0(data.dir, "covariates.rds"))) {
    covar <- read_rds(paste0(data.dir, "covariates.rds"))
  } else {
    
    # Note that elevation and flowlines data must be downloaded before running 
    # get_elevation() and get_flowlines(). See documentation for details.
    tri <- get_elevation(locs = region$sp.grid, path = "../species-futures/data/USA/", id.label = "conus.grid.id")
    stream <- get_flowlines(locs = region$sp.grid, path = "../species-futures/data/USA/", id.label = "conus.grid.id")
    landcover <- get_landcover(locs = region$sp.grid, path = "../species-futures/data/USA/", id.label = "conus.grid.id")
    
    climate <- get_climate(locs = region$sp.grid, id.label = "conus.grid.id")
    traveltime <- get_traveltime(locs = region$sp.grid, id.label = "conus.grid.id")
    
    
    
    # Combine
    covar <- full_join(tri, stream, by = "conus.grid.id") %>%
      full_join(landcover, by = "conus.grid.id") %>%
      full_join(climate, by = "conus.grid.id") %>%
      full_join(traveltime, by = "conus.grid.id") %>%
      select(conus.grid.id, streamLength.km, prec, forest, elevation, traveltime)
    
    covar <- covar[order(match(covar$conus.grid.id, region$sp.grid$conus.grid.id)),]
    write_rds(covar, file = paste0(data.dir, "covariates.rds"))
    
    
  }
}
```

Next we remove grid cells that do not have complete covariates.

```{r incomplete-covs}
## Remove incomplete cases
rm <- which(complete.cases(covar[,covs.z]) == F)
if (length(rm) > 0) {
  covar <- covar[-rm,]
  region$sp.grid <- region$sp.grid[-rm,]
}
```

Then we scale all covariates to have a mean of 1 and a standard deviation of 0.

```{r scale-covs}
## Scale covariates 
covar_unscaled <- covar
numcols <- sapply(covar, is.numeric)
numcols <- which(numcols)
covar[,numcols] <- sapply(covar[,numcols], scale_this)
```

Finally we create the quadratic covariates (as defined in `model-specs.csv`).

```{r quad-covs}
if (length(covs.quad) > 0 & paste0(covs.quad, collapse = "") != "") {
  for (c in 1:length(covs.quad)) {
    covar[,paste0(covs.quad[c], "2")] <- covar[,covs.quad[c]] * covar[,covs.quad[c]]
    covs.z <- c(covs.z, paste0(covs.quad[c], "2"))
  }
}
```


There's one additional step that could be done here. If removing multicollinearity is desired, the following piece of code would check and remove covariates that violate a certain correlation threshold (we use 0.4). If there are fewer than 3 covariates remaining after the procedure, then a message will alert you.

```{r collinearity, eval=F}
# remove covariates that are correlated > 0.4
if (check.covs == T){
  
  threshold <- 0.4
  
  covs.rm <- select_covar(covs.z, threshold = threshold)
  covs.lin <- covs.lin[-which(covs.lin %in% covs.rm)]
  covs.quad <- covs.quad[-which(covs.quad %in% covs.rm)]
  
  covs.z <- c(covs.lin, covs.quad)
}

if (length(covs.z) < 3) {
  stop("There are fewer than 3 covariates remaining! This probably isn't a good model")
}
```


## Step 6: Plot covariates

Now that we've downloaded covariate data, let's take a look at the distribution of each covariate and the correlations between them.

```{r plot-covs, results = 'hide'}

# ISSUE RILEY - functions don't auto-save figures anymore, do we want to save the figures and then display them from their file location or just display them from within r
if (fold.out == "none") {
  
  ## Distribution covariates
  
  ## Define covariate labels
  covlabs <- read.csv("data/covariate-labels.csv") %>% filter(covariate %in% covs.z)
  
  out <- plot_covar(covar,
                    region,
                    cov.labs = covlabs,
                    scaled = T)
  
  out <- cor_covar(covar, 
                   cov.labs = covlabs,
                   color.threshold = 0.25)
  
  ## iNat covariates
  if ("iNaturalist" %in% names(species.data$obs)) {
    ## Define covariate labels
    covlabs <- read.csv("data/covariate-labels.csv") %>%
      filter(covariate %in% covs.inat)
    
    out <- plot_covar(covar,
                      region,
                      cov.labs = covlabs,
                      scaled = T)
    
    if (length(covs.inat) > 1) {
      out <- cor_covar(covar, 
                       cov.labs = covlabs,
                       color.threshold = 0.25)
    }
  }
  
  ## PO covs
  
  ## Define covariate labels
  covlabs <- read.csv("data/covariate-labels.csv") %>%
    filter(covariate %in% covs.PO)
  
  out <- plot_covar(covar,
                    region,
                    cov.labs = covlabs,
                    scaled = T)
  
  if (length(covs.PO) > 1) {
    out <- cor_covar(covar, 
                     cov.labs = covlabs,
                     color.threshold = 0.25)
  }
}
```


![Distribution of distribution-level covariates across the subrange](~/GitHub/iSDM-framework/outputs/3_RACA_subrange/1_covariates-a_process-map.jpg)

![Correlation between distribution-level covariates](~/GitHub/iSDM-framework/outputs/3_RACA_subrange/1_covariates-a_process-correlations.jpg)

![Distribution of presence-only (PO) effort covariates](~/GitHub/iSDM-framework/outputs/3_RACA_subrange/1_covariates-c_PO-map.jpg)

## Step 7: NIMBLE

To load the data into NIMBLE, the data must be formatted in a specific way. We have wrapped this process in a function that combines and formats species datasets (`allfiles`) and their associated covariates (contained in `allfiles`, `covs.inat`, and `covs.PO`)

```{r spdata}
sp.data <- sppdata_for_nimble(
  species.data,
  region,
  file.info = allfiles,
  covar = covar,
  covs.inat = covs.inat,
  covs.PO = covs.PO,
  DND.maybe = 1,  # treat "maybe" detections as 1 or 0?
  # Only keep PO cells that training grid cells
  keep.conus.grid.id = gridkey$conus.grid.id[which(gridkey$group == "train")]) 
```

Next, the distribution-level covariates (`covar`) are combined with the formatted species data. This combined data is then broken into `data` and `constants` which are the inputs for the NIMBLE model. As their names imply, the `data` list object contains values which vary over space or time (e.g., species data, distribution-level covariates, and detection covariates). The `constants` list object contains fixed values such as the number of cells in the grid, grid indices for specific datasets, names of datasets, and parameters for the spatial model.

```{r data-constants}
tmp <- data_for_nimble(sp.data, covar = covar, covs.z,
                       sp.auto = sp.auto, coarse.grid = coarse.grid, 
                       region = region, process.intercept = F,
                       gridkey = gridkey, spatRegion = spatRegion)

data <- tmp$data
constants <- tmp$constants
```


The model structure allows for an indicator variable to indicate whether a cell is in a state with PO effort or not. There are generally two cases where this is useful: 

- If a state agency has collected PO only from within its state, by definition there is effort within that state but not outside of it. This information is already stored in `constants`, which pulled it from `allfiles$POextent`
- In iNaturalist data, if a species has taxon geoprivacy (i.e., obscured locations due to listing status) within a state, all records from that state will be removed by `load_species_data()` due to high coordinate uncertainty. The effort in that state is therefore effectively 0. 

The following code adds the state indicator variable to `constants`.

```{r state-indicator}

# GPOR has taxon geoprivacy in four states
if (sp.code == "GPOR") obsc.state <- c("CT", "MS", "NJ", "RI")

# RACA has no iNat data
if (sp.code == "RACA") obsc.state <- NA

constants <- add_state_ind(
  species.data,
  region,
  gridkey,
  constants,
  stategrid = stategrid,
  obsc.state = obsc.state,
  keep.conus.grid.id = gridkey$conus.grid.id[which(gridkey$group == "train")])
```


Once `data` and `constants` are created for NIMBLE, the function `nimble_code()` writes the NIMBLE code using the information contained in those objects as well several other specificiations defined earlier in the code. The NIMBLE code is saved as a .R file in the location defined by the `path` argument, and in the object `code`.


```{r code}
code <- nimble_code(data,
                    constants, 
                    path = out.dir,
                    sp.auto = sp.auto, 
                    coarse.grid = coarse.grid,
                    Bprior = Bprior,
                    block.out = fold.out,
                    zero_mean = zero_mean,
                    rm.state = F,
                    tau = tau)
```

Next, set up the initial values, also based on the information in `data` and `constants`.

```{r inits}
inits <- function(x){nimble_inits(data,
                                  constants,
                                  sp.auto = sp.auto,
                                  seed = x)}
```

Finally, identify for which parameters the model should save chains. By default, distribution model parameters ($\beta$) and dataset intercepts ($\alpha$) are saved. Parameters that are estimated for each hexbin ($\lambda$, $XB$, effort) require more storage, so there are options to disable them. If `sp.auto = T`, then the spatial random effect and $\tau$ are also saved.

```{r params}
params <- nimble_params(data,
                        constants,
                        lambda = T,
                        XB = T,
                        effort = T,
                        sp.auto = sp.auto)
```


## Step 8: Clean up and save

Remove objects that are large and are not required for further steps, save the workspace, and move on to `02-flexiSDM.R`.

```{r clean-up}
# Remove local and fold in case the setup is run locally but the model is fit on the HPC.
# Remove other unnecessary files to reduce the size of setup_BLOCK.Rdata
rm(list=c('local','fold','args','conus.covar.grid','conus.grid', 'conus.huc',
          'usa','conus', 'pl','centroid'))


# Save environment and full set up
save.image(paste0(out.dir, "setup_",fold.out,".Rdata"))
```



# Part 2: 02-flexiSDM.R

The goal of this script is solely to fit the model via NIMBLE. The code is set up to allow for parallelization of the MCMC chains so they can be fit simultaneously on either a local computer or a high performance computing system.


Because this script can be run independently of `01-flexiSDM.R`, we set some key parameters manually and load the setup file created previously.

```{r param.edit2}
num <- 4 # model number
fold <- "none" # CV fold

mods <- read.csv("code/model-specs.csv") %>% filter(number %in% num) # get species code and model name from csv file
sp.code <- mods$sp.code[1] # species code
model <- mods$model[1] # model name

# Set output directory and load setup file
out.dir = paste0('outputs/',num,'_',sp.code,'_',model,'/')
load(paste0(out.dir,'setup_',fold,'.Rdata'))


local <- 1 # are you fitting the model locally (1) or on an HPC (0)?
chain <- 1 # chain number (only needed if fitting as an array job on an HPC)
```


```{r load-packages2, results = 'hide'}
# Load functions and packages
source("code/FXN-nimbleParallel.R") 
library(SpFut.flexiSDM)
```



## Fit NIMBLE model

If you intend to fit the model locally, the function `nimbleParallel()` will create a local cluster on your computer with a default of 3 cores (this can be changed using the `cores` argument). It then configures, compiles, and runs the MCMC in parallel for the number of chains you have defined. The output is then saved as a single output object. 

If you intend to use an HPC to fit the model, please see the section below on high performance computing.

```{r fit-nimble}
if (local == 1) {
  start.nim <- Sys.time()
  samples <- nimbleParallel(code = code,
                            data = data,
                            constants = constants,
                            inits = inits,
                            param = params,
                            iter = iter,
                            burnin = burnin,
                            thin = thin)
  
  end.nim <- Sys.time() - start.nim
  print(end.nim)
  
  saveRDS(samples, paste0(out.dir,'samples_',fold,'.rds'))
} else {
  
  info <- list(seed = chain,
               inits = inits(chain))
  
  start.nim <- Sys.time()
  samples <- run_nimbleMCMC(info = info,
                            code = code,
                            constants = constants,
                            data = data,
                            param = params,
                            iter = iter,
                            burnin = burnin,
                            thin = thin)
  
  end.nim <- Sys.time() - start.nim
  print(end.nim)
  
  saveRDS(samples, paste0(out.dir,'samples_',fold,'_',chain,'.rds'))
}
```



# Part 3: 03-flexiSDM.R

The goal of this script is to summarize the chains from the NIMBLE MCMC model and visualize the output in a series of figures. 

Because this script can be run independently of `02-flexiSDM.R`, we set load packages, set some key parameters manually, and load the setup file created in the first script before proceeding.


```{r load-packages3, results = 'hide', eval = F}
## load packages ----
library(tidyverse, quietly = T)
library(magrittr, quietly = T)
library(sf, quietly = T)
library(SpFut.flexiSDM)
```


```{r params.edit3}
num <- 4 # model number
fold <- "none" # CV fold

mods <- read.csv("code/model-specs.csv") %>% filter(number %in% num) # get species code and model name from csv file
sp.code <- mods$sp.code[1] # species code
model <- mods$model[1] # model name

# Set output directory and load setup file
out.dir = paste0('outputs/',num,'_',sp.code,'_',model,'/')
load(paste0(out.dir,'setup_',fold,'.Rdata'))

local <- 1 # are you fitting the model locally (1) or on an HPC (0)?
maxchain <- 3 # the total number of chains that were fit
```


First we'll load the MCMC samples created by `02-flexiSDM.R`, assuming the model was fit on your local computer. Instructions for models fit with a high performance computing are below. To save time in fitting the model and reduce the size of the samples.rds file, we removed all derived quantities (e.g., occupancy probability ($\psi$) and projections) from the NIMBLE code. We can calculate those derived quantities now by using the `get_derived()` function, which will create a derived quantity for each step in the MCMC chain.

# ERROR iN SECTION BELOW
```{r load-chains, eval=F}
# Load chains (samples) ----
samples <- readRDS(paste0(out.dir,'samples_',fold,'.rds'))

## Calculate derived quantities ----
samples <- lapply(samples, get_derived, data = data, project = project,
                  coarse.grid = coarse.grid, spatRegion = spatRegion)
```

Our next step is to summarize the MCMC chains using the `summarize_samples` function. This function also creates a local cluster on your machine to parallelize the summarization. In this step, you will be summarizing a matrix of 

```{r summarize-chains, eval = F}
## Summarize chains ----
out <- summarize_samples(samples,
                         data,
                         constants,
                         project = project,
                         coarse.grid = coarse.grid,
                         block.out = fold.out,
                         gridkey = gridkey,
                         spatkey = spatRegion$spatkey,
                         effort = T,
                         SLURM = ifelse(local == 1, F, T))

save(out, file = paste0(out.dir, "data", foldname, ".rdata"))
```


```{r plot-output, eval=F}
# Plot output ----

# Convergence
ggsave(plot_convergence(out),
       file = paste0(out.dir, "3_parameters-0_convergence-", fold, ".jpg"),
       height = 5, width = 12)

if (fold.out == "none") {
  cov.labs <- read.csv("data/covariate-labels.csv")
  
  ### Chains ----
  ggsave(plot_chains(samples, data = data, cov.labs = cov.labs,
                     plot = "B", cutoff = 0),
         file = paste0(out.dir, "3_parameters-a1_chains-B.jpg"),
         height = 6, width = 8)
  ggsave(plot_chains(samples, data = data, cov.labs = cov.labs,
                     plot = "alpha", cutoff = 0),
         file = paste0(out.dir, "3_parameters-b1_chains-alpha.jpg"),
         height = 6, width = 8)
  ggsave(plot_chains(samples, data = data, cov.labs = cov.labs,
                     plot = "tau", cutoff = 0),
         file = paste0(out.dir, "3_parameters-b1_chains-tau-",fold,".jpg"),
         height = 6, width = 8)
  
  # Posteriors ----
  ggsave(plot_posteriors(samples, data = data, cov.labs = cov.labs,
                         plot = "B", cutoff = 0),
         file = paste0(out.dir, "3_parameters-a2_posteriors-B.jpg"),
         height = 6, width = 8)
  ggsave(plot_posteriors(samples, data = data, cov.labs = cov.labs,
                         plot = "alpha", cutoff = 0),
         file = paste0(out.dir, "3_parameters-b2_posteriors-alpha.jpg"),
         height = 6, width = 8)
  
  
  
  ### Parameter estimates ----
  ggsave(plot_pars(out,
                   plot.type = "full",
                   plot.group = "process",
                   title = "Process parameter estimates",
                   cov.labs = cov.labs),
         file = paste0(out.dir, "3_parameters-a3_B.jpg"),
         height = 6, width = 10)
  
  ggsave(plot_pars(out,
                   plot.type = "full",
                   plot.group = "dataset",
                   title = "Dataset parameter estimates"),
         file = paste0(out.dir, "3_parameters-b3_alpha.jpg"),
         height = 6, width = 10)
  
  ggsave(plot_pars(out,
                   plot.type = "full",
                   plot.group = "observation",
                   title = "Observation intercept estimates",
                   cov.labs = cov.labs),
         file = paste0(out.dir, "3_parameters-c3_observation.jpg"),
         height = 6, width = 10)
  
  ggsave(plot_pars(out,
                   plot.type = "full",
                   plot.group = "tau",
                   title = "Tau estimate"),
         file = paste0(out.dir, "3_parameters-c3_tau.jpg"),
         height = 6, width = 10)
  
  ### Marginal effects ----
  ggsave(plot_effects(data, out, breaks = 0.001),
         file = paste0(out.dir, "/3_parameters-a4-effects.jpg"),
         height = 7, width = 10)
  
  
  ### Maps ----
  # Current
  pl <- map_species_data(title = paste0(common, " (", sp.code, ")"),
                         region,
                         plot = "lambda",
                         out = out)
  ggsave(pl, file = paste0(out.dir, "4_mapcurrent-a_lambda.jpg"),
         height = 7, width = 9)
  
  pl <- map_species_data(title = paste0(common, " (", sp.code, ")"),
                         region,
                         plot = "lambda",
                         out = out,
                         plot.uncertainty = "unc.range")
  ggsave(pl, file = paste0(out.dir, "4_mapcurrent-a_lambda-uncabs.jpg"),
         height = 7, width = 9)
  
  pl <- map_species_data(title = paste0(common, " (", sp.code, ")"),
                         region,
                         plot = "lambda",
                         out = out,
                         plot.uncertainty = "unc.rel")
  ggsave(pl, file = paste0(out.dir, "4_mapcurrent-a_lambda-uncrel.jpg"),
         height = 7, width = 9)
  
  pl <- map_species_data(title = paste0(common, " (", sp.code, ")"),
                         region,
                         plot = "psi",
                         out = out)
  ggsave(pl, file = paste0(out.dir, "4_mapcurrent-b_psi.jpg"),
         height = 7, width = 9)
  
  pl <- map_species_data(title = paste0(common, " (", sp.code, ")"),
                         region,
                         plot = "psi",
                         out = out,
                         plot.uncertainty = "unc.range")
  ggsave(pl, file = paste0(out.dir, "4_mapcurrent-b_psi-uncabs.jpg"),
         height = 7, width = 9)
  
  pl <- map_species_data(title = paste0(common, " (", sp.code, ")"),
                         region,
                         plot = "psi",
                         out = out,
                         plot.uncertainty = "unc.rel")
  ggsave(pl, file = paste0(out.dir, "4_mapcurrent-b_psi-uncrel.jpg"),
         height = 7, width = 9)
  
  pl <- map_species_data(title = paste0(common, " (", sp.code, ")"),
                         region,
                         plot = "boundary",
                         out = out,
                         threshold = 0.25)
  ggsave(pl, file = paste0(out.dir, "4_mapcurrent-c1_boundary-0.25.jpg"),
         height = 7, width = 9)
  
  pl <- map_species_data(title = paste0(common, " (", sp.code, ")"),
                         region,
                         plot = "boundary",
                         out = out,
                         threshold = 0.5)
  ggsave(pl, file = paste0(out.dir, "4_mapcurrent-c2_boundary-0.5.jpg"),
         height = 7, width = 9)
  
  pl <- map_species_data(title = paste0(common, " (", sp.code, ")"),
                         region,
                         plot = "boundary",
                         out = out,
                         threshold = 0.75)
  ggsave(pl, file = paste0(out.dir, "4_mapcurrent-c3_boundary-0.75.jpg"),
         height = 7, width = 9)
  
  pl <- map_species_data(title = paste0(common, " (", sp.code, ")"),
                         region,
                         plot = "spat",
                         out = out,
                         coarse.grid = coarse.grid,
                         spatRegion = spatRegion)
  ggsave(pl, file = paste0(out.dir, "4_mapcurrent-d_spat.jpg"),
         height = 7, width = 9)
  
  pl <- map_species_data(title = paste0(common, " (", sp.code, ")"),
                         region,
                         plot = "XB",
                         out = out,
                         plot.exp = T)
  ggsave(pl, file = paste0(out.dir, "4_mapcurrent-d_expXB.jpg"),
         height = 7, width = 9)
  
  pl <- map_species_data(title = paste0(common, " (", sp.code, ")"),
                         region,
                         plot = "effort",
                         out = out)
  ggsave(pl, file = paste0(out.dir, "4_mapcurrent-e_effort.jpg"),
         height = 7, width = 9)
  
  
  ### Future projections ----
  if (project > 0) {
    pl <- map_species_data(title = paste0(common, " (", sp.code, ")"),
                           region,
                           plot = "lambda",
                           out = out,
                           plot.current = F,
                           plot.change = F,
                           proj.names = proj.names)
    ggsave(pl, file = paste0(out.dir, "5_mapfuture-a_lambda.jpg"),
           height = 7, width = 9)
    
    pl <- map_species_data(title = paste0(common, " (", sp.code, ")"),
                           region,
                           plot = "lambda",
                           out = out,
                           plot.current = F,
                           plot.change = F,
                           plot.uncertainty = "unc.range",
                           proj.names = proj.names)
    ggsave(pl, file = paste0(out.dir, "5_mapfuture-a_lambda-uncabs.jpg"),
           height = 7, width = 9)
    
    pl <- map_species_data(title = paste0(common, " (", sp.code, ")"),
                           region,
                           plot = "lambda",
                           out = out,
                           plot.current = F,
                           plot.change = F,
                           plot.uncertainty = "unc.rel",
                           proj.names = proj.names)
    ggsave(pl, file = paste0(out.dir, "5_mapfuture-a_lambda-uncrel.jpg"),
           height = 7, width = 9)
    
    pl <- map_species_data(title = paste0(common, " (", sp.code, ")"),
                           region,
                           plot = "psi",
                           out = out,
                           plot.current = F,
                           plot.change = F,
                           proj.names = proj.names)
    ggsave(pl, file = paste0(out.dir, "5_mapfuture-b_psi.jpg"),
           height = 7, width = 9)
    
    pl <- map_species_data(title = paste0(common, " (", sp.code, ")"),
                           region,
                           plot = "psi",
                           out = out,
                           plot.current = F,
                           plot.change = F,
                           plot.uncertainty = "unc.range",
                           proj.names = proj.names)
    ggsave(pl, file = paste0(out.dir, "5_mapfuture-b_psi-uncabs.jpg"),
           height = 7, width = 9)
    
    pl <- map_species_data(title = paste0(common, " (", sp.code, ")"),
                           region,
                           plot = "psi",
                           out = out,
                           plot.current = F,
                           plot.change = F,
                           plot.uncertainty = "unc.rel",
                           proj.names = proj.names)
    ggsave(pl, file = paste0(out.dir, "5_mapfuture-b_psi-uncrel.jpg"),
           height = 7, width = 9)
    
    pl <- map_species_data(title = paste0(common, " (", sp.code, ")"),
                           region,
                           plot = "boundary",
                           out = out,
                           plot.current = F,
                           plot.change = F,
                           threshold = 0.25,
                           proj.names = proj.names)
    ggsave(pl, file = paste0(out.dir, "5_mapfuture-c1_boundary-0.25.jpg"),
           height = 7, width = 9)
    
    pl <- map_species_data(title = paste0(common, " (", sp.code, ")"),
                           region,
                           plot = "boundary",
                           out = out,
                           plot.current = F,
                           plot.change = F,
                           threshold = 0.5,
                           proj.names = proj.names)
    ggsave(pl, file = paste0(out.dir, "5_mapfuture-c2_boundary-0.5.jpg"),
           height = 7, width = 9)
    
    pl <- map_species_data(title = paste0(common, " (", sp.code, ")"),
                           region,
                           plot = "boundary",
                           out = out,
                           plot.current = F,
                           plot.change = F,
                           threshold = 0.75,
                           proj.names = proj.names)
    ggsave(pl, file = paste0(out.dir, "5_mapfuture-c3_boundary-0.75.jpg"),
           height = 7, width = 9)
    
    pl <- map_species_data(title = paste0(common, " (", sp.code, ")"),
                           region,
                           plot = "XB",
                           out = out,
                           plot.exp = T,
                           plot.current = F,
                           proj.names = proj.names)
    ggsave(pl, file = paste0(out.dir, "5_mapfuture-d_expXB-future.jpg"),
           height = 7, width = 9)
    
    
    ### Future absolute change ----
    pl <- map_species_data(title = paste0(common, " (", sp.code, ")"),
                           region,
                           plot = "lambda",
                           out = out,
                           plot.current = F,
                           plot.change = "absolute",
                           proj.names = proj.names)
    ggsave(pl, file = paste0(out.dir, "6_mapchange-a1_lambda-abs.jpg"),
           height = 7, width = 9)
    
    pl <- map_species_data(title = paste0(common, " (", sp.code, ")"),
                           region,
                           plot = "psi",
                           out = out,
                           plot.current = F,
                           plot.change = "absolute",
                           proj.names = proj.names)
    ggsave(pl, file = paste0(out.dir, "6_mapchange-b1_psi-abs.jpg"),
           height = 7, width = 9)
    
    pl <- map_species_data(title = paste0(common, " (", sp.code, ")"),
                           region,
                           plot = "boundary",
                           out = out,
                           plot.current = F,
                           plot.change = "absolute",
                           threshold = 0.25,
                           proj.names = proj.names)
    ggsave(pl, file = paste0(out.dir, "6_mapchange-c1_boundary-abs-0.25.jpg"),
           height = 7, width = 9)
    
    pl <- map_species_data(title = paste0(common, " (", sp.code, ")"),
                           region,
                           plot = "boundary",
                           out = out,
                           plot.current = F,
                           plot.change = "absolute",
                           threshold = 0.5,
                           proj.names = proj.names)
    ggsave(pl, file = paste0(out.dir, "6_mapchange-c2_boundary-abs-0.5.jpg"),
           height = 7, width = 9)
    
    pl <- map_species_data(title = paste0(common, " (", sp.code, ")"),
                           region,
                           plot = "boundary",
                           out = out,
                           plot.current = F,
                           plot.change = "absolute",
                           threshold = 0.75,
                           proj.names = proj.names)
    ggsave(pl, file = paste0(out.dir, "6_mapchange-c3_boundary-abs-0.75.jpg"),
           height = 7, width = 9)
    
    ### Future relative change ----
    pl <- map_species_data(title = paste0(common, " (", sp.code, ")"),
                           region,
                           plot = "lambda",
                           out = out,
                           plot.current = F,
                           plot.change = "relative",
                           proj.names = proj.names)
    ggsave(pl, file = paste0(out.dir, "6_mapchange-a2_lambda-rel.jpg"),
           height = 7, width = 9)
    
    pl <- map_species_data(title = paste0(common, " (", sp.code, ")"),
                           region,
                           plot = "psi",
                           out = out,
                           plot.current = F,
                           plot.change = "relative",
                           proj.names = proj.names)
    ggsave(pl, file = paste0(out.dir, "6_mapchange-b2_psi-rel.jpg"),
           height = 7, width = 9)
  }
}
```


































# High performance computing

We almost exclusively run these models on a high performance computing (HPC) cluster. We clone our repository directly to the HPC to ensure we have the same file structure and no version conflicts. We've set up the code so we can seamlessly move between running the workflow locally and on our HPC. 

To run the first steps of the workflow via `01-flexiSDM.R`, we call the script `01-flexiSDM.sh`, which takes 3 inputs: 

- Model number
- fold (defined by an array in the script)
- Local (1/0)

We have simplified this further by writing a wrapper called `01-HPC.sh`, which only takes one input: model number. Within the script, `local` is set to 0, indicating a run on an HPC.

The inputs provided to the `.sh` scripts are converted to inputs to the `01-flexiSDM.R` script and are interpreted below:

```{r hpc, eval = F}
# Converts command arguments to something interpretable by R
args = commandArgs(trailingOnly = TRUE)

if (length(args) > 0) {
  
  # Model number to run
  nums.do = as.numeric(args[1])
  
  # Blocks can be run as single jobs or as arrays
  fold = as.numeric(args[2])
  
  # The fold input is numeric only. Convert 4 = 'none'
  if (fold == 4) {
    fold <- 'none'
  }
  
  # Running locally? Yes = 1
  local = as.numeric(args[3])
  
  # If running on an HPC, set the working directory to the project home directory
  if (local == 0) {
    setwd('/home/directory/')
  }
}
```




<!-- args = commandArgs(trailingOnly = TRUE) -->

<!-- if (length(args) > 0) { -->
<!--   num = as.numeric(args[1]) -->
<!--   sp.code = as.character(args[2]) -->
<!--   model = as.character(args[3]) -->
<!--   fold = as.numeric(args[4]) -->
<!--   chain = as.numeric(args[5]) -->
<!--   local = as.numeric(args[6])  -->

<!--   # If running on HPC, set working directory -->
<!--   if (local == 0) { -->
<!--     setwd('/caldera/hovenweep/projects/usgs/ecosystems/eesc/rmummah/species-futures/') -->
<!--   }  -->
<!-- } -->

<!-- # Ensure fold is coded correctly -->
<!-- if (fold == 4) { -->
<!--   fold <- 'none' -->
<!-- } -->




<!-- # Setup ---- -->
<!-- args = commandArgs(trailingOnly = TRUE) -->

<!-- if (length(args) > 0) { -->
<!--   num = as.numeric(args[1]) -->
<!--   sp.code = as.character(args[2]) -->
<!--   model = as.character(args[3]) -->
<!--   fold = as.numeric(args[4]) -->
<!--   maxchain = as.numeric(args[5]) -->
<!--   local = as.numeric(args[6]) -->

<!--   if (local == 0) { -->
<!--     setwd('/caldera/hovenweep/projects/usgs/ecosystems/eesc/rmummah/species-futures/') -->
<!--   }  -->

<!-- } -->

<!-- # Ensure fold is coded correctly -->
<!-- if (fold == 4) { -->
<!--   fold <- 'none' -->
<!-- } -->